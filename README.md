Unsupervised machine learning models are created from the wine data for clustering from Kaggle.com. First by reducing dimensionality with PCA at 80% variance, then carrying out K-means with 3 clusters using the elbow and silhouette methods and finally by carrying out hierarchical clustering for comparison to K-means.
Instructions: Download the  Wine Data for Clustering available on Kaggle.com. Conduct PCA to collapse correlated variables into a subset that includes 80% of the variance of the entirety of the data.  Then conduct k-means to identify clusters, and evaluate different values for k (e.g., 3, 4,â€¦).  Finally, conduct hierarchical clustering.   Investigate assumptions.  Interpret all of your findings. 
In my research of wine data clustering with data provided by Kaggle.com, research starts by dimensionality reduction using Principal Component Analysis (PCA) to collapse correlated variables into a subset that includes 80% of variance of the entirety of the data. After conducting PCA, K-means clustering using 4 and then 3 clusters via the elbow and silhouette methods are employed. Results show better separation of clusters with 3 versus 4 clusters. Finally hierarchical clustering using the ward method is carried out as a third unsupervised machine learning method. Hierarchical clustering also achieves 3 main clusters just like in K-means using PCA with 80% variance. 
According to Kannan and Menaga (2025), in hierarchical clustering, similar objects are clustered into groups using either the distance matrix or raw data. The algorithm works by first detecting the two clusters closest together. Secondly, the hierarchical clustering algorithm fuses the two most analogous groups and continues this process until all the clusters are combined. The output, which is a dendrogram, shows the categorized relationship between clusters and the dimension of the straight line between two different groups indicates the Euclidian  distance between the groups. 
Sung (2025) argues that dimensionality reduction helps mitigate potential challenges caused by high dimensional features. Examples provided are overfitting, computational inefficiencies and noisy patterns. The author argues that Principal Component Analysis (PCA) is employed as an unsupervised linear transformation method that identifies orthogonal directions in the data. Known as principal components. There are several benefits of PCA that are achieved: firstly, is variance retention whereby the value of k is chosen based on the cumulative explained variance ratio that ensures that 90% of the original variance is preserved . This approach apparently strikes a good balance between complexity reduction and information retention. This compares well with the 80% variance on PCA using the wine data research. The second benefit of PCA is noise reduction, this is because principal  components with negligible variance are eliminated. In so doing, PCA acts as a form of noise filtering which can improve the subsequent model performance.
In another study, Shang (2025) defines  K-means algorithm as a distance based clustering algorithm, which divides the samples in the dataset into k clusters. The algorithm makes the samples in the same cluster more similar, while the samples between different clusters are less similar. K-means clustering helps to find potential structures and patterns in the data and thus provides valuable information for sports injury prediction. There, however, exists disadvantages to K-means algorithm in that it is vulnerable to the problem of data imbalance. This occurs  when the number of samples of one class in the data set is far more than that of other classes. This has an effect in  the clustering. The LOF algorithm can identify outliers in the dataset by calculating the local outlier factor (LOF) of each data point. The LOF algorithm can evaluate the degree of anomaly of the point relative to its local neighborhood class and if the LOF value of a point is far greater than 1, it is considered as an outlier. Removal of these outliers before clustering K-means clustering process, thus improving the accuracy of clustering.
References:
Kannan, K., & Menaga, A. (2025). An efficient approach on risk factor prediction related to cardiovascular disease around Kumbakonam, Tamil Nadu, India, using unsupervised machine learning techniques. Scientific Reports, 15(1), 5369. https://doi.org/10.1038/s41598-025-89403-4
Shang, T. (2025). Improvement of key feature mining algorithm for sports injury data based on LOF enhanced k-means and sparse PCA. Informatica, 49(8). https://doi.org/10.31449/inf.v49i8.7230
Sung, P. H. (2025). Advanced machine learning for housing market analysis: Predicting property prices in Washington, DC. Authorea Preprints. 1-8. https://doi.org/10.36227/techrxiv.173603502.26172421/v2



